{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4d0d29",
   "metadata": {},
   "source": [
    "## 多分类（MNIST）\n",
    "* 10个输出\n",
    "* 输出满足分布的要求，大于0，且所有和为1\n",
    "* 最后一层为Softmax层\n",
    "$$\n",
    "P(y=i)=\\frac{e^{z_i}}{\\sum_{j=0}^{K-1}e^{z_j}},i\\in {0,...,K-1}\n",
    "$$\n",
    "\n",
    "* loss函数，标签为one-hot\n",
    "* NLLLoss: Negative Log Likelihood Loss $-Ylog\\hat{Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644f391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729189131256584\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y=np.array([1,0,0])\n",
    "z=np.array([0.2,0.1,-0.1])\n",
    "y_pred=np.exp(z)/np.exp(z).sum()\n",
    "loss=(-y*np.log(y_pred)).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c6ecd",
   "metadata": {},
   "source": [
    "Pytorch中直接将z和y求损失：torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99eb9b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4966) tensor(1.2389)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "y=torch.LongTensor([2,0,1]) # 变成长整型Tensor\n",
    "\n",
    "y_pred1 = torch.Tensor([[0.1,0.2,0.9],\n",
    "                      [1.1,0.1,0.2],\n",
    "                      [0.2,2.1,0.1]])\n",
    "\n",
    "y_pred2 = torch.Tensor([[0.8,0.2,0.3],\n",
    "                      [0.2,0.3,0.5],\n",
    "                      [0.2,0.2,0.5]])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "l1 = criterion(y_pred1,y)\n",
    "l2 = criterion(y_pred2,y)\n",
    "\n",
    "print(l1.data,l2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dff0a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:51.47%\n",
      "Accuracy on test set:55.54%\n",
      "Accuracy on test set:56.86%\n",
      "Accuracy on test set:68.06%\n",
      "Accuracy on test set:68.72%\n",
      "Accuracy on test set:68.82%\n",
      "Accuracy on test set:68.99%\n",
      "Accuracy on test set:68.98%\n",
      "Accuracy on test set:69.09%\n",
      "Accuracy on test set:69.16%\n",
      "Accuracy on test set:69.11%\n",
      "Accuracy on test set:69.15%\n",
      "Accuracy on test set:69.15%\n",
      "Accuracy on test set:69.13%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.2%\n",
      "Accuracy on test set:69.12%\n",
      "Accuracy on test set:69.15%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.27%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.27%\n",
      "Accuracy on test set:69.19%\n",
      "Accuracy on test set:69.27%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.2%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.19%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.2%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.29%\n",
      "Accuracy on test set:69.27%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.18%\n",
      "Accuracy on test set:69.18%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.19%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.19%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.22%\n",
      "Accuracy on test set:69.25%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.27%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.2%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.23%\n",
      "Accuracy on test set:69.26%\n",
      "Accuracy on test set:69.24%\n",
      "Accuracy on test set:69.21%\n",
      "Accuracy on test set:69.17%\n"
     ]
    }
   ],
   "source": [
    "# MNIST数据集分类\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "# 1.数据准备\n",
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,),(0.3081))]) #通道(0.1307,)均值，通道(0.3081)标准差\n",
    "train_dataset = torchvision.datasets.MNIST(root = \"./dataset/mnist\", train=True, download=True,transform=transform)\n",
    "train_loader = DataLoader(train_dataset,shuffle=True,batch_size=batch_size)\n",
    "test_dataset = torchvision.datasets.MNIST(root = \"./dataset/mnist\", train=False, download=True,transform=transform)\n",
    "test_loader = DataLoader(test_dataset,shuffle=False,batch_size=batch_size)\n",
    "\n",
    "# 构造模型\n",
    "# 2. 构造模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__() #调用父类的构造，必须要有\n",
    "        self.linear1 = torch.nn.Linear(784,512) #Linear对象中包含了weight和bias这两个Tensor,自动实现wx+b\n",
    "        self.linear2 = torch.nn.Linear(512,256)\n",
    "        self.linear3 = torch.nn.Linear(256,128)\n",
    "        self.linear4 = torch.nn.Linear(128,64)\n",
    "        self.linear5 = torch.nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,784)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# 3. 构造损失函数和优化器\n",
    "# 这里损失函数用BCE   \n",
    "criterion = torch.nn.CrossEntropyLoss() #对于输入z，做softmax,Log,-YlogY\n",
    "\n",
    "# optim中有一个类叫SGD torch.optim.SGD() weight_decay(加一个w^Tw的优化目标)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.5)#告诉优化器对哪些Tensor做梯度优化，由model中的paramenters告知\n",
    "\n",
    "# 4. 训练周期\n",
    "# 把一轮epoch封装成一个函数\n",
    "def train(epoch):\n",
    "    running_loss = 0\n",
    "    for batch_idx,data in enumerate(train_loader,0):\n",
    "        inputs,target = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss +=loss.item()\n",
    "        \n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): #主要是用于停止autograd模块的工作,以起到加速和节省显存的作用\n",
    "        for data in test_loader:\n",
    "            images,labels = data\n",
    "            outputs = model(images)\n",
    "            _,predicted = torch.max(outputs.data,dim=1) #返回最大值及其索引\n",
    "            total +=labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy on test set:{100*correct/total}%\")\n",
    "    \n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf339cde",
   "metadata": {},
   "source": [
    "图像：\n",
    "* W*H*C 宽*高*通道数，一般要转成C*W*H\n",
    "* 取值在[0,255]，一般将其压缩成[0,1]\n",
    "* 使用transforms.Compose来是实现\n",
    "\n",
    "图像需要从矩阵拼接成向量x.view(-1,784) 28*28=784,-1代表不确定的数\n",
    "\n",
    "当使用冲量momentum时，则把每次x的更新量v考虑为本次的梯度下降量- dx * lr与上次x的更新量v乘上一个介于[0,1]的因子momentum的和，即v = - dx * lr + v * momemtum。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88d810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
