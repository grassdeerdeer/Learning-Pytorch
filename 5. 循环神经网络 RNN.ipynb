{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6f9788",
   "metadata": {},
   "source": [
    "# RNN\n",
    "* RNN（Recurrent Neural Network）是一类用于处理序列数据的神经网络。比如股票。\n",
    "* 设输入为x，输出为h（称为hidden隐藏向量）\n",
    "* Pytorch中有一个torch.nn.RNNCell(input_size=input_size,hidden_size=hidden_size)\n",
    "\n",
    "* RNNCell的维度设置\n",
    "    * input.shape = (batchSize,inputSize)\n",
    "    * output.shape = (batchSize,hiddenSize)\n",
    "    * dataset.shape = (seqLen,batchSize,inputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd2177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n",
      "tensor([[ 0.6245, -0.1693]], grad_fn=<TanhBackward0>)\n",
      "==================== 1 ====================\n",
      "tensor([[ 0.1703, -0.2498]], grad_fn=<TanhBackward0>)\n",
      "==================== 2 ====================\n",
      "tensor([[0.7344, 0.0335]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 如何使用RNNCell\n",
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "\n",
    "cell = torch.nn.RNNCell(input_size=input_size,hidden_size=hidden_size)\n",
    "\n",
    "dataset = torch.randn(seq_len, batch_size, input_size)\n",
    "hidden = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "for idx,inputs in enumerate(dataset):\n",
    "    print('='*20,idx,'='*20)\n",
    "    hidden = cell(inputs,hidden)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3fee",
   "metadata": {},
   "source": [
    "* RNN的维度设置\n",
    "    * input\n",
    "        * input.shape = (seqSize,batchSize,inputSize)\n",
    "        * hidden.shape = (numLayers,batchSize,hiddenSize)\n",
    "    * output\n",
    "        * output.shape = (seqLen,batchSize,hiddenSize)\n",
    "        * hidden.shape = (numLayers,batchSize,inputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418e4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size torch.Size([3, 1, 2])\n",
      "Output: tensor([[[ 0.1688, -0.7854]],\n",
      "\n",
      "        [[ 0.9801, -0.9822]],\n",
      "\n",
      "        [[ 0.9556, -0.6337]]], grad_fn=<StackBackward0>)\n",
      "Hidden size: torch.Size([1, 1, 2])\n",
      "Hidden: tensor([[[ 0.9556, -0.6337]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 如何使用RNN\n",
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "\n",
    "# num_layers 表示叠加多少RNNCell; 输入为序列数据；不用自己写循环\n",
    "# 设置batchSize=True，调换input,output中的Tensor中的batchSize和seqLen中的顺序\n",
    "cell = torch.nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)\n",
    "\n",
    "inputs = torch.randn(seq_len, batch_size, input_size) # 不变\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "out,hidden = cell(inputs,hidden)\n",
    "\n",
    "print(\"Output size\", out.shape)\n",
    "print(\"Output:\",out)\n",
    "print(\"Hidden size:\",hidden.shape)\n",
    "print(\"Hidden:\",hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "663bc803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4])\n",
      "Predicted string ooooh，Epoch[1/15] loss=6.678467273712158\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ooool，Epoch[2/15] loss=5.518108367919922\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string oolol，Epoch[3/15] loss=4.819676399230957\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ollll，Epoch[4/15] loss=4.426673412322998\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ollll，Epoch[5/15] loss=4.118429660797119\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[6/15] loss=3.732583999633789\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[7/15] loss=3.3179757595062256\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[8/15] loss=3.0362939834594727\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[9/15] loss=2.8954944610595703\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[10/15] loss=2.8190925121307373\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[11/15] loss=2.754319190979004\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[12/15] loss=2.683513641357422\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[13/15] loss=2.6091785430908203\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[14/15] loss=2.537820339202881\n",
      "torch.Size([5, 1, 4])\n",
      "Predicted string ohlll，Epoch[15/15] loss=2.473862886428833\n"
     ]
    }
   ],
   "source": [
    "# 使用RNNCell学习序列到序列的转化规律: hello->ohlol\n",
    "# e:0 h:1 l:2 o:3 ; one hot\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "\n",
    "idx2char=['o','h','l','o']\n",
    "x_data = [1,0,2,2,3]\n",
    "y_data = [3,1,2,3,2]\n",
    "\n",
    "one_hot_lookup = [[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] # seq x inputsize\n",
    "inputs = torch.Tensor(x_one_hot).view(-1,batch_size,input_size)\n",
    "labels = torch.LongTensor(y_data).view(-1,1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,batch_size):\n",
    "        super(Model,self).__init__() #调用父类的构造，必须要有\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnncell = torch.nn.RNNCell(input_size=self.input_size,hidden_size=self.hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs,hidden):\n",
    "        hidden = self.rnncell(inputs,hidden)\n",
    "        return hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.batch_size,self.hidden_size) #创建零矩阵\n",
    "\n",
    "model = Model(input_size,hidden_size,batch_size)\n",
    "\n",
    "# 3. 构造损失函数和优化器\n",
    "# 这里损失函数用BCE   \n",
    "criterion = torch.nn.CrossEntropyLoss() #对于输入z，做softmax,Log,-YlogY\n",
    "\n",
    "# optim中有一个类叫SGD torch.optim.SGD() weight_decay(加一个w^Tw的优化目标)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    hidden = model.init_hidden()\n",
    "    print(inputs.shape)\n",
    "    print('Predicted string',end=\" \")\n",
    "    for inpu,label in zip(inputs,labels): #如果Inputs和循环变量一样，就会不断分解\n",
    "        hidden = model(inpu,hidden)\n",
    "        \n",
    "        loss += criterion(hidden,label)\n",
    "        _,idx = hidden.max(dim = 1)\n",
    "        print(idx2char[idx.item()],end=\"\")\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"，Epoch[{epoch+1}/15] loss={loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3160ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string oholl，Epoch[1/15] loss=1.133691668510437\n",
      "Predicted string oooll，Epoch[2/15] loss=0.9743810892105103\n",
      "Predicted string ooool，Epoch[3/15] loss=0.895417332649231\n",
      "Predicted string oolol，Epoch[4/15] loss=0.8364354372024536\n",
      "Predicted string ohlol，Epoch[5/15] loss=0.7770026326179504\n",
      "Predicted string ohlol，Epoch[6/15] loss=0.7153452038764954\n",
      "Predicted string ohlol，Epoch[7/15] loss=0.6577935218811035\n",
      "Predicted string ohlol，Epoch[8/15] loss=0.6076319813728333\n",
      "Predicted string ohlol，Epoch[9/15] loss=0.5650906562805176\n",
      "Predicted string ohlol，Epoch[10/15] loss=0.5289137959480286\n",
      "Predicted string ohlol，Epoch[11/15] loss=0.4975467622280121\n",
      "Predicted string ohlol，Epoch[12/15] loss=0.4698221683502197\n",
      "Predicted string ohlol，Epoch[13/15] loss=0.4468842148780823\n",
      "Predicted string ohlol，Epoch[14/15] loss=0.42747926712036133\n",
      "Predicted string ohlol，Epoch[15/15] loss=0.4092729091644287\n"
     ]
    }
   ],
   "source": [
    "# 使用RNN学习序列到序列的转化规律: hello->ohlol\n",
    "# e:0 h:1 l:2 o:3 ; one hot\n",
    "\n",
    "# * input\n",
    "#         * input.shape = (seqSize,batchSize,inputSize)\n",
    "#         * hidden.shape = (numLayers,batchSize,hiddenSize)\n",
    "#     * output\n",
    "#         * output.shape = (seqLen,batchSize,hiddenSize)\n",
    "#         * hidden.shape = (numLayers,batchSize,inputSize)\n",
    "\n",
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "\n",
    "idx2char=['o','h','l','o']\n",
    "x_data = [1,0,2,2,3]\n",
    "y_data = [3,1,2,3,2]\n",
    "\n",
    "one_hot_lookup = [[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] # seq x inputsize\n",
    "inputs = torch.Tensor(x_one_hot).view(-1,batch_size,input_size)\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,batch_size,num_layers):\n",
    "        super(Model,self).__init__() #调用父类的构造，必须要有\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = torch.nn.RNN(input_size=self.input_size,\n",
    "                                    hidden_size=self.hidden_size,\n",
    "                                   num_layers=num_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        hidden = torch.zeros(self.num_layers,\n",
    "                            self.batch_size,\n",
    "                            self.hidden_size)\n",
    "        out,_ = self.rnn(inputs,hidden)\n",
    "        return out.view(-1,hidden_size) # seq*batchsize,hiddensize\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.batch_size,self.hidden_size) #创建零矩阵\n",
    "\n",
    "model = Model(input_size,hidden_size,batch_size,num_layers)\n",
    "\n",
    "# 3. 构造损失函数和优化器\n",
    "# 这里损失函数用BCE   \n",
    "criterion = torch.nn.CrossEntropyLoss() #对于输入z，做softmax,Log,-YlogY\n",
    "\n",
    "# optim中有一个类叫SGD torch.optim.SGD() weight_decay(加一个w^Tw的优化目标)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    # print(outputs.shape)\n",
    "    loss = criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _,idx = outputs.max(dim = 1)\n",
    "    idx = idx.data.numpy()\n",
    "    \n",
    "    print('Predicted string', ''.join([idx2char[x] for x in idx]),end=\"\")\n",
    "    \n",
    "    print(f\"，Epoch[{epoch+1}/15] loss={loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0b655",
   "metadata": {},
   "source": [
    "# 添加Embedding和Linear layer\n",
    "one-hot编码的缺陷\n",
    "* 高纬度\n",
    "* 稀疏\n",
    "* 认为设定\n",
    "\n",
    "希望找到这样的编码：Embedding\n",
    "* 低维\n",
    "* 稠密\n",
    "* 从数据中学习到的\n",
    "\n",
    "Embedding:\n",
    "* inputSize * EmbeddingSize(之前是one-hot)\n",
    "* torch.nn.Embedding(size_of_the_dictionary,size_of_each_embedding_vector)\n",
    "* 输入是LongTensor，输出是(*,embedding_dim)*是输入形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39ad85fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string loooo，Epoch[1/15] loss=1.44900643825531\n",
      "Predicted string loooo，Epoch[2/15] loss=1.19287109375\n",
      "Predicted string lolol，Epoch[3/15] loss=0.9887218475341797\n",
      "Predicted string lhlol，Epoch[4/15] loss=0.8155452609062195\n",
      "Predicted string ohlhl，Epoch[5/15] loss=0.679765522480011\n",
      "Predicted string ohlol，Epoch[6/15] loss=0.5524497032165527\n",
      "Predicted string ohlol，Epoch[7/15] loss=0.43207812309265137\n",
      "Predicted string ohlol，Epoch[8/15] loss=0.3315155804157257\n",
      "Predicted string ohlol，Epoch[9/15] loss=0.2460474967956543\n",
      "Predicted string ohlol，Epoch[10/15] loss=0.17854580283164978\n",
      "Predicted string ohlol，Epoch[11/15] loss=0.12948141992092133\n",
      "Predicted string ohlol，Epoch[12/15] loss=0.09429500252008438\n",
      "Predicted string ohlol，Epoch[13/15] loss=0.06889984011650085\n",
      "Predicted string ohlol，Epoch[14/15] loss=0.050606053322553635\n",
      "Predicted string ohlol，Epoch[15/15] loss=0.03748921677470207\n"
     ]
    }
   ],
   "source": [
    "# 使用RNN学习序列到序列的转化规律: hello->ohlol\n",
    "# e:0 h:1 l:2 o:3 ; one hot\n",
    "\n",
    "# * input\n",
    "#         * input.shape = (seqSize,batchSize,inputSize)\n",
    "#         * hidden.shape = (numLayers,batchSize,hiddenSize)\n",
    "#     * output\n",
    "#         * output.shape = (seqLen,batchSize,hiddenSize)\n",
    "#         * hidden.shape = (numLayers,batchSize,inputSize)\n",
    "\n",
    "import torch\n",
    "\n",
    "num_class = 4\n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "embedding_size = 10\n",
    "num_layers = 2\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "\n",
    "\n",
    "idx2char=['o','h','l','o']\n",
    "x_data = [[1,0,2,2,3]] # (batch*seq_len)\n",
    "y_data = [3,1,2,3,2] # (batch*seq_len)\n",
    "\n",
    "inputs = torch.LongTensor(x_data)\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__() #调用父类的构造，必须要有\n",
    "    \n",
    "        \n",
    "        self.emb = torch.nn.Embedding(input_size,embedding_size)\n",
    "        self.rnn = torch.nn.RNN(input_size=embedding_size,\n",
    "                                    hidden_size=hidden_size,\n",
    "                                   num_layers=num_layers,\n",
    "                               batch_first=True) \n",
    "        self.fc = torch.nn.Linear(hidden_size,num_class) #Linear 仅是对输入的最后一维做线性变换,(输入维度，输出维度)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        hidden = torch.zeros(num_layers,\n",
    "                            x.size(0),\n",
    "                            hidden_size)\n",
    "        x = self.emb(x) #(batch,seqLen,embeddingSize)\n",
    "        x,_ = self.rnn(x,hidden)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1,num_class) # seq*batchsize,num_class\n",
    "    \n",
    "\n",
    "model = Model()\n",
    "\n",
    "# 3. 构造损失函数和优化器\n",
    "# 这里损失函数用BCE   \n",
    "criterion = torch.nn.CrossEntropyLoss() #对于输入z，做softmax,Log,-YlogY\n",
    "\n",
    "# optim中有一个类叫SGD torch.optim.SGD() weight_decay(加一个w^Tw的优化目标)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.05)\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    # print(outputs.shape)\n",
    "    loss = criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    _,idx = outputs.max(dim = 1)\n",
    "    idx = idx.data.numpy()\n",
    "    \n",
    "    print('Predicted string', ''.join([idx2char[x] for x in idx]),end=\"\")\n",
    "    \n",
    "    print(f\"，Epoch[{epoch+1}/15] loss={loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09163dda",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "* 其实就是增加了一个$c_t$通路，反向传播的时候路径比较直接\n",
    "* input(h_0,c_0)\n",
    "* output(h_n,c_n)\n",
    "\n",
    "GRU是LSTM和RNN在性能和计算复杂度之间的折衷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0eacf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import csv\n",
    "import time\n",
    "def time_since(since):\n",
    "    s = time.time()-since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return f'{m}minute,{s}second'\n",
    "\n",
    "def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88b64691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self,is_train_set=True):\n",
    "        filename = r\"./dataset/names_train.csv.gz\" if is_train_set else r'./dataset/names_test.csv.gz'\n",
    "        with gzip.open(filename,'rt') as f: #默认，只读文本\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]\n",
    "        self.len = len(self.names)\n",
    "        self.countries = [row[1] for row in rows]\n",
    "        \n",
    "        self.country_list = list(sorted(set(self.countries)))\n",
    "        self.country_dict = self.getCountryDict()\n",
    "        self.country_num = len(self.country_list)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.names[index],self.country_dict[self.countries[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()\n",
    "        for idx,country_name in enumerate(self.country_list,0):\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "    \n",
    "    def idx2country(self,index):\n",
    "        return self.country_list[index]\n",
    "    \n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed3ca86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,n_layers=1,bidirectional = True):\n",
    "        super(RNNClassifier,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(input_size,hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size,hidden_size,n_layers,\n",
    "                                bidirectional=bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size*self.n_directions,output_size)\n",
    "        pass\n",
    "    \n",
    "    def _init_hidden(self,batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*self.n_directions,\n",
    "                            batch_size,\n",
    "                            self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "    \n",
    "    def forward(self,inputs,seq_lengths):\n",
    "        # input shape: B*S-》S*B\n",
    "        inputs = inputs.t() #转置\n",
    "        batch_size = inputs.size(1)\n",
    "        \n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(inputs) # (seqlen,batchsize,hiddensize)\n",
    "        \n",
    "        # 返回一个 PackedSequence 对象，它主要包含两部分：data 和 batch_sizes ,batch_sizes实际上就是告诉网络每个时间步需要吃进去多少数据\n",
    "        # input：经过 pad_sequence 处理之后的数据。\n",
    "        # lengths：mini-batch中各个序列的实际长度。\n",
    "        # 默认条件下，我们必须把输入数据按照序列长度从大到小排列后才能送入 pack_padded_sequence ，否则会报错。\n",
    "        # 在 PyTorch 里面使用函数 pad_sequence 对序列进行填充。填充之后的样本序列，虽然长度相同了，但是序列里面可能填充了很多无效值 0 ，将填充值 0 喂给 RNN 进行 forward 计算，不仅浪费计算资源，最后得到的值可能还会存在误差。\n",
    "        gru_input = pack_padded_sequence(embedding,seq_lengths)\n",
    "    \n",
    "        \n",
    "        output,hidden = self.gru(gru_input,hidden)\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1],hidden[-2]],dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2list(name):\n",
    "    arr = [ord(c) for c in name] # ord返回对应的 ASCII 数值\n",
    "    return arr,len(arr)\n",
    "\n",
    "\n",
    "def make_tensors(names,countries):\n",
    "    sequences_and_lengths = [name2list(name) for name in names]\n",
    "    name_sequences = [s1[0] for s1 in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([s1[1] for s1 in sequences_and_lengths])\n",
    "    countries = countries.long()\n",
    "    \n",
    "    # batchsize * seqlen\n",
    "    seq_tensor = torch.zeros(len(name_sequences),seq_lengths.max()).long()\n",
    "    for idx,(seq,seq_len) in enumerate(zip(name_sequences,seq_lengths),0):\n",
    "        seq_tensor[idx,:seq_len]=torch.LongTensor(seq)\n",
    "        \n",
    "    # 排序\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0,descending=True)\n",
    "    seq_tensor =  seq_tensor[perm_idx]\n",
    "    countries = countries[perm_idx]\n",
    "    \n",
    "    return create_tensor(seq_tensor),create_tensor(seq_lengths),create_tensor(countries)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8753b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "Accuracy on test set:66.34328358208955%\n",
      "Accuracy on test set:73.67164179104478%\n",
      "Accuracy on test set:77.53731343283582%\n",
      "Accuracy on test set:79.53731343283582%\n",
      "Accuracy on test set:81.07462686567165%\n",
      "Accuracy on test set:82.28358208955224%\n",
      "Accuracy on test set:82.67164179104478%\n",
      "Accuracy on test set:83.19402985074628%\n",
      "Accuracy on test set:83.80597014925372%\n",
      "Accuracy on test set:84.13432835820896%\n",
      "Accuracy on test set:84.55223880597015%\n",
      "Accuracy on test set:84.3731343283582%\n",
      "Accuracy on test set:84.80597014925372%\n",
      "Accuracy on test set:84.65671641791045%\n",
      "Accuracy on test set:85.01492537313433%\n",
      "Accuracy on test set:84.49253731343283%\n",
      "Accuracy on test set:84.41791044776119%\n",
      "Accuracy on test set:84.44776119402985%\n",
      "Accuracy on test set:84.53731343283582%\n",
      "Accuracy on test set:84.26865671641791%\n",
      "Accuracy on test set:84.6268656716418%\n",
      "Accuracy on test set:84.43283582089552%\n",
      "Accuracy on test set:84.41791044776119%\n",
      "Accuracy on test set:83.94029850746269%\n",
      "Accuracy on test set:84.04477611940298%\n",
      "Accuracy on test set:84.2089552238806%\n",
      "Accuracy on test set:83.97014925373135%\n",
      "Accuracy on test set:83.7910447761194%\n",
      "Accuracy on test set:84.01492537313433%\n",
      "Accuracy on test set:84.1044776119403%\n",
      "Accuracy on test set:83.95522388059702%\n",
      "Accuracy on test set:84.07462686567165%\n",
      "Accuracy on test set:84.08955223880596%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-01d33ef71801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0macc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-01d33ef71801>\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\27586\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\27586\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\27586\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    142\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\27586\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 循环神经网络的分类器：根据名字，输出国家(18个)\n",
    "# 输入->embedding->GRU Layer -> 最后一个输入线形层\n",
    "# 字符串长度不一，以最长字符串为基准，其它字符串用0填充\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "n_chars=128 #字母表有多少元素\n",
    "hidden_size=100 #GRU\n",
    "n_layer=2 #GRU几层的\n",
    "batch_size=256\n",
    "N_EPOCHS = 100\n",
    "USE_GPU = False\n",
    "\n",
    "# 1. 数据准备\n",
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "testset = NameDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "n_country = trainset.getCountriesNum()\n",
    "    \n",
    "model = RNNClassifier(n_chars,hidden_size,n_country,n_layer)\n",
    "\n",
    "if USE_GPU:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    classifier.to(device)\n",
    "\n",
    "# 3. 构造损失函数和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss() #对于输入z，做softmax,Log,-YlogY\n",
    "\n",
    "# optim中有一个类叫SGD torch.optim.SGD() weight_decay(加一个w^Tw的优化目标)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "strat = time.time()\n",
    "print(f\"Training for {N_EPOCHS} epochs...\")\n",
    "acc_list = []\n",
    "\n",
    "\n",
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i,(names,countries) in enumerate(trainloader,1):\n",
    "        inputs, seq_lengths, target = make_tensors(names,countries)\n",
    "        output = model(inputs,seq_lengths)\n",
    "        loss = criterion(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    with torch.no_grad(): #主要是用于停止autograd模块的工作,以起到加速和节省显存的作用\n",
    "        for i,(names,countries) in enumerate(testloader,1):\n",
    "            inputs, seq_lengths, target = make_tensors(names,countries)\n",
    "            output = model(inputs,seq_lengths)\n",
    "            \n",
    "            pred = output.max(dim=1,keepdim=True)[1] #保持原数组的维度\n",
    "         \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() #返回被视作与给定的tensor相同大小的原tensor\n",
    "        print(f\"Accuracy on test set:{100*correct/total}%\")\n",
    "    return correct/total\n",
    "        \n",
    "for epoch in range(1,N_EPOCHS+1):\n",
    "    trainModel()\n",
    "    acc = testModel()\n",
    "    acc_list.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09188110",
   "metadata": {},
   "source": [
    "所有序列数据的问题，都可以用RNN去解决"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107408d",
   "metadata": {},
   "source": [
    "* 文档\n",
    "* 多读文献\n",
    "* 多动手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31954ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
